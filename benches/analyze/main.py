#!/usr/bin/env python3
# Generates markdown tables from criterion textual output.
# Outputs to stdout or to the README.md file if provided as an argument.

import os
import re
import sys

import matplotlib.pyplot as plt
import numpy as np
from tabulate import tabulate

MARKER = "<!-- AUTOGENERATED MARKER -->\n"
KINDS = ["lex", "parse"]


def main():
    """Main function to process benchmark data and generate output."""
    out_file = sys.argv[1] if len(sys.argv) > 1 else None

    # Read and preprocess input
    lines = read_input()
    if not lines:
        return

    # Extract benchmark information
    benchmarks = extract_benchmarks(lines)
    if not benchmarks:
        return

    # Extract timing data
    data, min_times = extract_timing_data(lines, benchmarks)
    parsers = list(sorted(set(x[1] for x in data)))

    # Apply solc base overhead correction
    apply_solc_correction(data)

    # Generate output
    out_s = ""

    # Generate plots
    plot_paths = plot_benchmark_times(data, benchmarks, parsers)

    # Add plot images to the output
    out_s += "### Charts\n\n"
    out_s += f"#### Lexing Performance\n\n![Lexing Performance]({os.path.basename(plot_paths['lex'])})\n\n"
    out_s += f"#### Parsing Performance\n\n![Parsing Performance]({os.path.basename(plot_paths['parse'])})\n\n"

    # Add relative performance charts
    out_s += f"#### Lexing Relative Performance\n\n![Lexing Relative Performance]({os.path.basename(plot_paths['lex_relative'])})\n\n"
    out_s += f"#### Parsing Relative Performance\n\n![Parsing Relative Performance]({os.path.basename(plot_paths['parse_relative'])})\n\n"

    # Generate markdown tables
    out_s += generate_markdown_tables(data, benchmarks, parsers, min_times)

    out_s = out_s.rstrip()

    # Replace the marker with the new output, or print to stdout
    if out_file:
        with open(out_file, "r") as f:
            content = f.read()
        idx = content.index(MARKER) + len(MARKER)
        with open(out_file, "w") as f:
            f.write(content[:idx] + "\n" + out_s + "\n")
    else:
        print(out_s)


def read_input():
    """Read and preprocess input from stdin."""
    lines = sys.stdin.readlines()
    if not lines:
        return []

    # Skip header lines
    while lines and ":" not in lines[0]:
        lines = lines[1:]

    # Filter out warning lines
    lines = list(filter(lambda x: "Warning" not in x, lines))
    return lines


def extract_benchmarks(lines):
    """Extract benchmark information from input lines."""
    benchmark_re = re.compile(r"(\w+): (\d+) LoC, (\d+) bytes")
    benchmarks = []

    for line in lines:
        if line.strip() == "":
            break
        match = benchmark_re.match(line)
        if match:
            name, loc, bytes = match.groups()
            benchmarks.append((name, int(loc), int(bytes)))

    return benchmarks


def extract_timing_data(lines, benchmarks):
    """Extract timing data from input lines."""
    time = r"(\s*[\d\.]+ \w+)"
    data_re = re.compile(
        rf"parser/(\w+)/(\w+)/(\w+)\s*time:\s*\n?\s*\[{time}{time}{time}\]",
        flags=re.MULTILINE,
    )

    data = []
    min_times = {b[0]: {} for b in benchmarks}

    for match in data_re.findall("\n".join(lines)):
        bench_name, parser, kind, _time1, time2, _time3 = match
        time_ns = parse_time_s(time2)

        # Track minimum time for each benchmark and kind
        min_time_entry = min_times[bench_name].get(kind, [parser, time_ns])
        min_times[bench_name][kind] = [
            min_time_entry[0],
            min(min_time_entry[1], time_ns),
        ]

        data.append([bench_name, parser, kind, time2, time_ns])

    return data, min_times


def apply_solc_correction(data):
    """Apply correction to solc parser times to remove base overhead."""
    # Find base solc time from empty benchmark
    base_solc_ns = -1
    for entry in data:
        bench_name, parser, _, _, ns = entry
        if bench_name == "empty" and parser == "solc":
            base_solc_ns = ns
            break

    if base_solc_ns == -1:
        raise ValueError("Couldn't find base solc time")

    # Subtract base overhead (keeping 1us)
    base_solc_ns -= 1_000

    # Apply correction to all solc entries
    for i, entry in enumerate(data):
        _, parser, _, _, ns = entry
        if parser == "solc":
            data[i][4] -= base_solc_ns
            data[i][3] = format_ns(data[i][4])


def generate_markdown_tables(data, benchmarks, parsers, min_times):
    """
    Generate markdown tables for each benchmark and kind.

    Args:
        data: List of [bench_name, parser, kind, time_str, time_ns] entries
        benchmarks: List of (bench_name, loc, bytes) tuples
        parsers: List of parser names
        min_times: Dictionary of minimum times for each benchmark and kind

    Returns:
        String containing markdown tables
    """
    out_s = ""
    for bench_name, loc, bytes in benchmarks:
        out_s += f"### {bench_name} ({loc} LoC, {bytes} bytes)\n\n"

        for kind in KINDS:
            table = []
            table.append(
                [
                    "Parser",
                    "Relative",
                    "Time",
                    "LoC/s",
                    "Bytes/s",
                ]
            )

            for parser in parsers:
                related = next(
                    (x for x in data if x[0:3] == [bench_name, parser, kind]),
                    None,
                )
                if not related:
                    continue

                min_time = min_times[bench_name][kind][1]
                time_ns = related[4]
                relative = format_number(time_ns / min_time) + "x"
                time_s = related[3]
                loc_s = get_per_second(loc, time_ns)
                bytes_s = get_per_second(bytes, time_ns)
                table.append(
                    [
                        parser,
                        relative,
                        time_s,
                        loc_s,
                        bytes_s,
                    ]
                )

            table[1:] = sorted(table[1:], key=lambda x: float(x[1][:-1]))

            out_s += f"#### {kind.capitalize()}\n"
            out_s += tabulate(table, headers="firstrow", tablefmt="pipe")
            out_s += "\n\n"

    return out_s


def plot_benchmark_times(data, benchmarks, parsers):
    """
    Plot the parsing and lexing times on a log chart for all parsers.

    Args:
        data: List of [bench_name, parser, kind, time_str, time_ns] entries
        benchmarks: List of (bench_name, loc, bytes) tuples
        parsers: List of parser names

    Returns:
        Dictionary with paths to the generated plot images
    """
    # Filter out the "empty" benchmark
    benchmarks = [b for b in benchmarks if b[0] != "empty"]

    # Get unique benchmark names
    bench_names = sorted(set(b[0] for b in benchmarks))

    # Calculate average times for each parser and kind to sort them
    parser_avg_times = calculate_parser_avg_times(data, bench_names, parsers)

    # Sort parsers by average time (fastest last)
    sorted_parsers = {
        kind: list(
            reversed(
                sorted(
                    parsers, key=lambda p: parser_avg_times[kind].get(p, float("inf"))
                )
            )
        )
        for kind in KINDS
    }

    # Calculate average time for each benchmark to sort them
    bench_avg_times = calculate_benchmark_avg_times(data, bench_names, parsers)

    # Sort benchmarks by average time (fastest first)
    sorted_bench_names = {
        kind: sorted(
            bench_names, key=lambda b: bench_avg_times[kind].get(b, float("inf"))
        )
        for kind in KINDS
    }

    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    os.makedirs(output_dir, exist_ok=True)

    plot_paths = {}

    # Generate separate plots for each kind
    for kind in KINDS:
        # Generate absolute time plots
        plot_paths[kind] = create_plot(
            data,
            kind,
            sorted_bench_names[kind],
            sorted_parsers[kind],
            output_dir,
        )

        # Generate relative performance plots
        plot_paths[f"{kind}_relative"] = create_relative_plot(
            data,
            kind,
            sorted_bench_names[kind],
            sorted_parsers[kind],
            output_dir,
        )

    return plot_paths


def calculate_parser_avg_times(data, bench_names, parsers):
    """Calculate average times for each parser and kind."""
    parser_avg_times = {kind: {} for kind in KINDS}

    for kind in KINDS:
        for parser in parsers:
            times = []
            for bench_name in bench_names:
                entry = next(
                    (x for x in data if x[0:3] == [bench_name, parser, kind]), None
                )
                if entry:
                    times.append(entry[4])  # time_ns
            if times:
                parser_avg_times[kind][parser] = sum(times) / len(times)

    return parser_avg_times


def calculate_benchmark_avg_times(data, bench_names, parsers):
    """Calculate average times for each benchmark and kind."""
    bench_avg_times = {kind: {} for kind in KINDS}

    for kind in KINDS:
        for bench_name in bench_names:
            times = []
            for parser in parsers:
                entry = next(
                    (x for x in data if x[0:3] == [bench_name, parser, kind]), None
                )
                if entry:
                    times.append(entry[4])  # time_ns
            if times:
                bench_avg_times[kind][bench_name] = sum(times) / len(times)

    return bench_avg_times


def create_plot(data, kind, sorted_bench_names, sorted_parsers, output_dir):
    """Create a single plot for a specific kind."""
    # Create a figure
    plt.figure(figsize=(12, 8))
    ax = plt.gca()

    # Set up the plot
    ax.set_title(f"Time to {kind} (log scale)")
    ax.set_xlabel("Benchmark")
    ax.set_ylabel("Time (ns)")
    ax.set_yscale("log")

    # Set up x-axis positions
    x = np.arange(len(sorted_bench_names))
    width = 0.8 / len(sorted_parsers)

    # Plot each parser
    for i, parser in enumerate(sorted_parsers):
        times = []
        for bench_name in sorted_bench_names:
            # Find the time for this parser and benchmark
            entry = next(
                (x for x in data if x[0:3] == [bench_name, parser, kind]), None
            )
            if entry:
                times.append(entry[4])  # time_ns
            else:
                times.append(np.nan)  # Missing data

        # Plot the data
        ax.bar(x + i * width - 0.4 + width / 2, times, width, label=parser)

    # Set x-axis labels
    ax.set_xticks(x)
    ax.set_xticklabels(sorted_bench_names, rotation=45, ha="right")

    # Add grid for better readability
    ax.grid(True, axis="y", linestyle="--", alpha=0.7)

    # Add legend
    ax.legend()

    # Adjust layout to prevent label cutoff
    plt.tight_layout()

    # Save the figure
    output_path = os.path.join(output_dir, f"{kind}_benchmark_times.png")
    plt.savefig(output_path, dpi=300)
    print(f"{kind.capitalize()} plot saved to {output_path}")

    # Close the figure to free memory
    plt.close()

    return output_path


def create_relative_plot(data, kind, sorted_bench_names, sorted_parsers, output_dir):
    """
    Create a relative performance plot showing speedup factors compared to the slowest parser.

    This visualization better highlights the performance differences between parsers
    by showing how many times faster each parser is compared to the slowest one.
    """
    # Create a figure
    plt.figure(figsize=(12, 8))
    ax = plt.gca()

    # Set up the plot
    ax.set_title(f"Relative {kind} Performance (higher is better)")
    ax.set_xlabel("Benchmark")
    ax.set_ylabel("Speedup Factor (x faster than slowest)")

    # Set up x-axis positions
    x = np.arange(len(sorted_bench_names))
    width = 0.8 / len(sorted_parsers)

    # Find the slowest parser for each benchmark
    slowest_times = {}
    for bench_name in sorted_bench_names:
        max_time = 0
        for parser in sorted_parsers:
            entry = next(
                (x for x in data if x[0:3] == [bench_name, parser, kind]), None
            )
            if entry and entry[4] > max_time:
                max_time = entry[4]
        slowest_times[bench_name] = max_time

    # Plot each parser
    for i, parser in enumerate(sorted_parsers):
        speedups = []
        for bench_name in sorted_bench_names:
            # Find the time for this parser and benchmark
            entry = next(
                (x for x in data if x[0:3] == [bench_name, parser, kind]), None
            )
            if entry:
                # Calculate speedup factor compared to slowest parser
                speedup = slowest_times[bench_name] / entry[4]
                speedups.append(speedup)
            else:
                speedups.append(np.nan)  # Missing data

        # Plot the data
        ax.bar(x + i * width - 0.4 + width / 2, speedups, width, label=parser)

    # Set x-axis labels
    ax.set_xticks(x)
    ax.set_xticklabels(sorted_bench_names, rotation=45, ha="right")

    # Add grid for better readability
    ax.grid(True, axis="y", linestyle="--", alpha=0.7)

    # Add legend
    ax.legend()

    # Adjust layout to prevent label cutoff
    plt.tight_layout()

    # Save the figure
    output_path = os.path.join(output_dir, f"{kind}_relative_performance.png")
    plt.savefig(output_path, dpi=300)
    print(f"{kind.capitalize()} relative performance plot saved to {output_path}")

    # Close the figure to free memory
    plt.close()

    return output_path


def parse_time_s(time: str):
    """Parse a time string into nanoseconds."""
    value, unit = time.strip().split(" ")
    value = float(value)
    if unit == "s":
        return int(value * 1_000_000_000)
    elif unit == "ms":
        return int(value * 1_000_000)
    elif unit == "us" or unit == "µs":
        return int(value * 1_000)
    elif unit == "ns":
        return int(value * 1)
    else:
        raise ValueError(f"Unknown unit: {unit}")


def get_per_second(total: int, ns: int):
    """Calculate items per second based on total and nanoseconds."""
    if total == 0 or ns == -1:
        return "N/A"

    s = ns / 1_000_000_000
    return format_number(total / s)


def format_number(n: float):
    """Format a number with appropriate suffix (K, M, B)."""
    if n >= 1_000_000_000:
        n /= 1_000_000_000
        s = "B"
    elif n >= 1_000_000:
        n /= 1_000_000
        s = "M"
    elif n >= 1_000:
        n /= 1_000
        s = "K"
    else:
        s = ""
    return f"{n:.2f}{s}"


def format_ns(ns: int):
    """Format nanoseconds with appropriate unit (ns, µs, ms, s)."""
    if ns >= 1_000_000_000:
        ns /= 1_000_000_000
        s = "s"
    elif ns >= 1_000_000:
        ns /= 1_000_000
        s = "ms"
    elif ns >= 1_000:
        ns /= 1_000
        s = "µs"
    else:
        s = "ns"
    return f"{ns:f}"[:6] + " " + s


if __name__ == "__main__":
    main()
